#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI ë©”ë‰´ ê²€ìƒ‰ ì‹œìŠ¤í…œ
- ìë™ í˜ì´ì§€ ë¶„ë¥˜ + ë°ì´í„° ê°•í™” + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- ìŠ¤ë§ˆíŠ¸ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ì‘ì—… ë°©ì§€  
- OpenAI APIì™€ ìµœì‹  AI ê¸°ìˆ ì„ ê²°í•©í•œ ì°¨ì„¸ëŒ€ ê²€ìƒ‰ í”Œë«í¼
"""

import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
import os
from openai import OpenAI
import time
import re
from collections import Counter
import math

warnings.filterwarnings("ignore")

# SSL ì¸ì¦ì„œ ë¬¸ì œ í•´ê²°
import ssl
ssl._create_default_https_context = ssl._create_unverified_context


class BM25:
    """BM25 ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)"""
    
    def __init__(self, corpus, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.corpus = corpus
        self.doc_freqs = []
        self.idf = {}
        self.doc_lens = []
        self.avgdl = 0
        
        self._initialize()
    
    def _initialize(self):
        """BM25 ì´ˆê¸°í™”"""
        for doc in self.corpus:
            words = doc.lower().split()
            self.doc_lens.append(len(words))
            
            word_freq = Counter(words)
            self.doc_freqs.append(word_freq)
        
        self.avgdl = sum(self.doc_lens) / len(self.doc_lens)
        
        all_words = set()
        for doc_freq in self.doc_freqs:
            all_words.update(doc_freq.keys())
        
        for word in all_words:
            containing_docs = sum(1 for doc_freq in self.doc_freqs if word in doc_freq)
            self.idf[word] = math.log((len(self.corpus) - containing_docs + 0.5) / (containing_docs + 0.5))
    
    def get_scores(self, query):
        """ì¿¼ë¦¬ì— ëŒ€í•œ ëª¨ë“  ë¬¸ì„œì˜ BM25 ì ìˆ˜ ê³„ì‚°"""
        query_words = query.lower().split()
        scores = []
        
        for i, doc_freq in enumerate(self.doc_freqs):
            score = 0
            doc_len = self.doc_lens[i]
            
            for word in query_words:
                if word in doc_freq:
                    freq = doc_freq[word]
                    idf = self.idf.get(word, 0)
                    
                    numerator = freq * (self.k1 + 1)
                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)
                    score += idf * (numerator / denominator)
            
            scores.append(score)
        
        return scores


class UnifiedMenuSearcher:
    def __init__(self, openai_api_key=None):
        """í†µí•© ë©”ë‰´ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        self.client = OpenAI(api_key=openai_api_key or os.getenv("OPENAI_API_KEY"))
        
        # íŒŒì¼ ê²½ë¡œ ì •ì˜
        self.original_data_file = 'ia-data.json'
        self.filtered_data_file = 'ia-data_filtered.json'
        self.enhanced_data_file = 'ia-data_enhanced.json'
        
        # ë‹¨ê³„ë³„ ë°ì´í„° ë¡œë“œ
        self.original_data = self._load_original_data()
        self.filtered_data = self._load_or_create_filtered_data()
        self.enhanced_data = self._load_or_create_enhanced_data()
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
        
        # ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
        if not self.use_tfidf:
            self.embeddings = self._create_embeddings()
        else:
            self._setup_tfidf()
        
        self._setup_bm25()
    
    def _load_original_data(self):
        """ì›ë³¸ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(self.original_data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(data)}ê°œ í•­ëª©")
            return data
        except Exception as e:
            print(f"âŒ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _load_or_create_filtered_data(self):
        """í•„í„°ë§ëœ ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        if os.path.exists(self.filtered_data_file):
            try:
                with open(self.filtered_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"âœ… ê¸°ì¡´ í•„í„°ë§ëœ ë°ì´í„° ë¡œë“œ: {len(data)}ê°œ í•­ëª©")
                return data
            except Exception as e:
                print(f"âš ï¸ í•„í„°ë§ëœ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("ğŸ” í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        return self._create_filtered_data()
    
    def _load_or_create_enhanced_data(self):
        """ê°•í™”ëœ ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        if os.path.exists(self.enhanced_data_file):
            try:
                with open(self.enhanced_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"âœ… ê¸°ì¡´ ê°•í™”ëœ ë°ì´í„° ë¡œë“œ: {len(data)}ê°œ í•­ëª©")
                return data
            except Exception as e:
                print(f"âš ï¸ ê°•í™”ëœ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("ğŸš€ ê°•í™”ëœ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        return self._create_enhanced_data()
    
    def _create_filtered_data(self):
        """í˜ì´ì§€ ë¶„ë¥˜ë¥¼ í†µí•œ í•„í„°ë§ëœ ë°ì´í„° ìƒì„±"""
        print("ğŸ”§ ê·œì¹™ ê¸°ë°˜ í˜ì´ì§€ ë¶„ë¥˜ ì‹œì‘...")
        
        # ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ íŒ¨í„´
        exclude_keywords = [
            "ì˜¤ë¥˜", "ì‹¤íŒ¨", "ì—†ìŒ", "empty", "ì•ˆë‚´", "ë™ì˜", "ë¸Œë¦¿ì§€", "ì ê²€ì¤‘",
            "ì„¸ì…˜", "íƒ€ì„ì•„ì›ƒ", "ë¯¸ì—°ê²°", "ì—…ë°ì´íŠ¸", "ìŠ¤í”Œë˜ì‹œ", "ëœë”©", "ë³µí˜¸í™”",
            "íƒì§€", "ì¥ì• ", "ì¢…ë£Œ", "ì¬ì‹¤í–‰", "ê¶Œí•œ", "ìŠ¤ì¼ˆë ˆí†¤"
        ]
        
        error_patterns = [
            r".*ì˜¤ë¥˜.*", r".*ì‹¤íŒ¨.*", r".*ì—†ìŒ.*", r".*empty.*", r".*error.*",
            r".*fail.*", r".*timeout.*", r".*ì„¸ì…˜.*", r".*ì ê²€.*"
        ]
        
        bridge_patterns = [
            r".*ë¸Œë¦¿ì§€.*", r".*bridge.*", r".*ëœë”©.*", r".*landing.*",
            r".*ìŠ¤í”Œë˜ì‹œ.*", r".*splash.*"
        ]
        
        filtered_data = []
        
        for item in self.original_data:
            page_name = item.get('page_name', '').strip()
            category = item.get('Category', '')
            service = item.get('Service', '')
            hierarchy = item.get('hierarchy', [])
            
            # ê³µë°± í˜ì´ì§€ ì œì™¸
            if not page_name or page_name.isspace():
                continue
            
            # ì˜¤ë¥˜ í˜ì´ì§€ ì œì™¸
            is_error = any(re.match(pattern, page_name, re.IGNORECASE) for pattern in error_patterns)
            if is_error:
                continue
            
            # ë¸Œë¦¿ì§€ í˜ì´ì§€ ì œì™¸
            is_bridge = any(re.match(pattern, page_name, re.IGNORECASE) for pattern in bridge_patterns)
            if is_bridge:
                continue
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ì œì™¸
            is_excluded = any(keyword in page_name.lower() or keyword in service.lower() for keyword in exclude_keywords)
            if is_excluded:
                continue
            
            # ê³„ì¸µ ê¹Šì´ ì²´í¬ (ë„ˆë¬´ ê¹Šì€ ê²ƒì€ ì œì™¸)
            if len(hierarchy) > 5:
                continue
            
            # ìœ ì˜ë¯¸í•œ í˜ì´ì§€ë¡œ íŒë‹¨ë˜ë©´ ì¶”ê°€
            enhanced_item = item.copy()
            enhanced_item['is_meaningful'] = True
            enhanced_item['filter_reason'] = 'rules_based'
            filtered_data.append(enhanced_item)
        
        # ì €ì¥
        try:
            with open(self.filtered_data_file, 'w', encoding='utf-8') as f:
                json.dump(filtered_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… í•„í„°ë§ëœ ë°ì´í„° ì €ì¥: {self.filtered_data_file} ({len(filtered_data)}ê°œ í•­ëª©)")
        except Exception as e:
            print(f"âŒ í•„í„°ë§ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return filtered_data
    
    def _create_enhanced_data(self):
        """AIë¥¼ í™œìš©í•œ ê°•í™”ëœ ë°ì´í„° ìƒì„±"""
        print("ğŸ¤– AIë¥¼ í™œìš©í•´ ë°ì´í„°ë¥¼ ê°•í™”í•©ë‹ˆë‹¤...")
        print(f"ğŸ“Š ì´ {len(self.filtered_data)}ê°œ í•­ëª© ì²˜ë¦¬ ì˜ˆì •")
        
        enhanced_data = []
        
        for i, item in enumerate(self.filtered_data, 1):
            print(f"ğŸ”„ ì§„í–‰ì¤‘ [{i}/{len(self.filtered_data)}]: {item.get('page_name', 'Unknown')}")
            
            enhanced_item = item.copy()
            
            # ëŒ€í‘œ ê²€ìƒ‰ì–´ ìƒì„±
            enhanced_item['representative_keywords'] = self._generate_representative_keywords(item)
            
            # AI ì„¤ëª… ìƒì„±
            enhanced_item['ai_description'] = self._generate_ai_description(item)
            
            # í˜ì´ì§€ ë¶„ë¥˜
            enhanced_item['page_classification'] = self._classify_page_importance(item)
            
            enhanced_data.append(enhanced_item)
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ë”œë ˆì´
            if i % 10 == 0:
                print(f"â±ï¸ API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•´ ì ì‹œ ëŒ€ê¸°ì¤‘...")
                time.sleep(2)
            else:
                time.sleep(0.5)
        
        # ì €ì¥
        try:
            with open(self.enhanced_data_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ê°•í™”ëœ ë°ì´í„° ì €ì¥: {self.enhanced_data_file}")
        except Exception as e:
            print(f"âŒ ê°•í™”ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return enhanced_data
    
    def _generate_representative_keywords(self, item):
        """ëŒ€í‘œ ê²€ìƒ‰ì–´ ìƒì„±"""
        try:
            category = item.get('Category', '')
            service = item.get('Service', '')
            page_name = item.get('page_name', '')
            hierarchy = ' > '.join(item.get('hierarchy', []))
            
            menu_info = f"ì¹´í…Œê³ ë¦¬: {category}, ì„œë¹„ìŠ¤: {service}, í˜ì´ì§€ëª…: {page_name}, ê³„ì¸µêµ¬ì¡°: {hierarchy}"
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë©”ë‰´ ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë©”ë‰´ ì •ë³´ë¥¼ ë³´ê³ , ì‚¬ìš©ìê°€ ì´ ë©”ë‰´ë¥¼ ì°¾ê¸° ìœ„í•´ ê²€ìƒ‰í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²€ìƒ‰ì–´ 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ê° ê²€ìƒ‰ì–´ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒ ë©”ë‰´ì— ëŒ€í•œ ëŒ€í‘œ ê²€ìƒ‰ì–´ 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n{menu_info}"}
                ],
                max_tokens=100,
                temperature=0.3
            )
            
            keywords_text = response.choices[0].message.content.strip()
            keywords = [kw.strip() for kw in keywords_text.split(',')]
            return keywords[:5]
            
        except Exception as e:
            print(f"âŒ ëŒ€í‘œ ê²€ìƒ‰ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            return [page_name, category, service]
    
    def _generate_ai_description(self, item):
        """AI ì„¤ëª… ìƒì„±"""
        try:
            category = item.get('Category', '')
            service = item.get('Service', '')
            page_name = item.get('page_name', '')
            hierarchy = ' > '.join(item.get('hierarchy', []))
            
            menu_info = f"ì¹´í…Œê³ ë¦¬: {category}, ì„œë¹„ìŠ¤: {service}, í˜ì´ì§€ëª…: {page_name}, ê³„ì¸µêµ¬ì¡°: {hierarchy}"
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë©”ë‰´ ì„¤ëª… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë©”ë‰´ ì •ë³´ë¥¼ ë³´ê³ , ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í•œ ë¬¸ì¥ì˜ ì„¤ëª…ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”. 'ì´ í˜ì´ì§€ëŠ” ~ë¥¼ í•  ìˆ˜ ìˆëŠ” í™”ë©´ì…ë‹ˆë‹¤' í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒ ë©”ë‰´ì— ëŒ€í•œ ì„¤ëª…ì„ ìƒì„±í•´ì£¼ì„¸ìš”:\n{menu_info}"}
                ],
                max_tokens=100,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âŒ AI ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì´ í˜ì´ì§€ëŠ” {item.get('page_name', '')} ê´€ë ¨ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í™”ë©´ì…ë‹ˆë‹¤."
    
    def _classify_page_importance(self, item):
        """í˜ì´ì§€ ì¤‘ìš”ë„ ë¶„ë¥˜"""
        page_name = item.get('page_name', '')
        
        # í•µì‹¬ ê¸°ëŠ¥ í‚¤ì›Œë“œ
        primary_keywords = ['í™ˆ', 'ì¹´ë“œ', 'ê²°ì œ', 'ê´€ë¦¬', 'ì„¤ì •', 'ì¡°íšŒ', 'ë‚´ì—­', 'ëª…ì„¸ì„œ', 'í¬ì¸íŠ¸']
        
        if any(keyword in page_name for keyword in primary_keywords):
            return 'PRIMARY'
        else:
            return 'SECONDARY'
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        local_model_paths = [
            "./models/multilingual-e5-small",
            "./models/ko-sroberta-multitask",
            "./models/distiluse-base-multilingual-cased-v2",
            "./models/all-mpnet-base-v2",
        ]
        
        model_loaded = False
        self.current_model_name = "Unknown"
        
        for model_path in local_model_paths:
            if os.path.exists(model_path) and os.path.exists(f"{model_path}/pytorch_model.bin"):
                try:
                    model_name = os.path.basename(model_path)
                    print(f"ğŸ”„ {model_name} ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                    self.model = SentenceTransformer(model_path)
                    print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                    
                    self.use_tfidf = False
                    self.current_model_name = model_name
                    model_loaded = True
                    break
                except Exception as e:
                    print(f"âŒ {model_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        if not model_loaded:
            try:
                print("ì˜¨ë¼ì¸ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
                print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                self.use_tfidf = False
                self.current_model_name = "all-MiniLM-L6-v2"
            except Exception as e:
                print(f"ì˜¨ë¼ì¸ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ TF-IDFë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.use_tfidf = True
                self.current_model_name = "TF-IDF"
    
    def _create_enhanced_text_for_embedding(self, item):
        """ê°•í™”ëœ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±"""
        category = item.get('Category', '')
        service = item.get('Service', '')
        page_name = item.get('page_name', '')
        hierarchy = ' > '.join(item.get('hierarchy', []))
        
        basic_text = f"ì¹´í…Œê³ ë¦¬: {category} ì„œë¹„ìŠ¤: {service} í˜ì´ì§€ëª…: {page_name} ê³„ì¸µêµ¬ì¡°: {hierarchy}"
        
        # ëŒ€í‘œ ê²€ìƒ‰ì–´ ê°€ì¤‘ì¹˜
        representative_keywords = item.get('representative_keywords', [])
        if representative_keywords:
            keywords_text = ' '.join(representative_keywords)
            weighted_keywords = f" {keywords_text} {keywords_text} {keywords_text}"
        else:
            weighted_keywords = ""
        
        # AI ì„¤ëª… ê°€ì¤‘ì¹˜
        ai_description = item.get('ai_description', '')
        if ai_description:
            weighted_description = f" {ai_description} {ai_description}"
        else:
            weighted_description = ""
        
        return basic_text + weighted_keywords + weighted_description
    
    def _create_embeddings(self):
        """ê°•í™”ëœ ë²¡í„° ì„ë² ë”© ìƒì„±"""
        print("ğŸ”® ê°•í™”ëœ ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        enhanced_texts = [self._create_enhanced_text_for_embedding(item) for item in self.enhanced_data]
        page_texts = [item.get('page_name', '') for item in self.enhanced_data]
        ai_descriptions = [item.get('ai_description', '') for item in self.enhanced_data]
        
        embeddings = {
            'enhanced': self.model.encode(enhanced_texts, show_progress_bar=True),
            'page': self.model.encode(page_texts, show_progress_bar=True),
            'description': self.model.encode(ai_descriptions, show_progress_bar=True)
        }
        
        print("âœ… ê°•í™”ëœ ë²¡í„° ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        return embeddings
    
    def _setup_tfidf(self):
        """TF-IDF ì„¤ì •"""
        texts = [self._create_enhanced_text_for_embedding(item) for item in self.enhanced_data]
        self.tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
    
    def _setup_bm25(self):
        """BM25 ì¸ë±ìŠ¤ ì„¤ì •"""
        print("ğŸ” BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        bm25_texts = []
        for item in self.enhanced_data:
            text_parts = [
                item.get('page_name', ''),
                item.get('Category', ''),
                item.get('Service', ''),
                ' '.join(item.get('hierarchy', [])),
                ' '.join(item.get('representative_keywords', []))
            ]
            bm25_text = ' '.join(filter(None, text_parts))
            bm25_texts.append(bm25_text)
        
        self.bm25 = BM25(bm25_texts)
        print("âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def hybrid_search(self, query, top_k=7, vector_weight=0.7, keyword_weight=0.3):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° + í‚¤ì›Œë“œ"""
        results = []
        
        if self.use_tfidf:
            query_vector = self.tfidf_vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]
            
            for i, similarity in enumerate(similarities):
                results.append({
                    'index': i,
                    'data': self.enhanced_data[i],
                    'vector_score': float(similarity),
                    'keyword_score': 0.0,
                    'hybrid_score': float(similarity)
                })
        else:
            query_embedding = self.model.encode([query])
            vector_similarities = cosine_similarity(query_embedding, self.embeddings['enhanced'])[0]
            keyword_scores = self.bm25.get_scores(query)
            
            max_vector_score = max(vector_similarities) if max(vector_similarities) > 0 else 1
            max_keyword_score = max(keyword_scores) if max(keyword_scores) > 0 else 1
            
            for i, (vector_sim, keyword_score) in enumerate(zip(vector_similarities, keyword_scores)):
                normalized_vector = vector_sim / max_vector_score
                normalized_keyword = keyword_score / max_keyword_score
                
                hybrid_score = (normalized_vector * vector_weight) + (normalized_keyword * keyword_weight)
                
                results.append({
                    'index': i,
                    'data': self.enhanced_data[i],
                    'vector_score': float(vector_sim),
                    'keyword_score': float(keyword_score),
                    'hybrid_score': float(hybrid_score)
                })
        
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        return results[:top_k]
    
    def ai_final_selection(self, original_query, candidates):
        """AI ìµœì¢… ì„ íƒ"""
        try:
            menu_text = ""
            for i, result in enumerate(candidates, 1):
                item = result['data']
                ai_description = item.get('ai_description', '')
                menu_text += f"""{i}. {item.get('page_name', '')}
   ì¹´í…Œê³ ë¦¬: {item.get('Category', '')}
   ì„œë¹„ìŠ¤: {item.get('Service', '')}
   ì„¤ëª…: {ai_description}
   ëŒ€í‘œê²€ìƒ‰ì–´: {', '.join(item.get('representative_keywords', []))}
   
"""
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•´ ê°€ì¥ ì í•©í•œ ë©”ë‰´ë¥¼ ì„ íƒí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ê³¼ ê° ë©”ë‰´ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬, ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ê³  ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•  ìˆ˜ ìˆëŠ” ë©”ë‰´ í•˜ë‚˜ì˜ ë²ˆí˜¸ë§Œ ë‹µí•˜ì„¸ìš”. 
ë°˜ë“œì‹œ ìˆ«ìë§Œ ë‹µí•˜ì„¸ìš”."""},
                    {"role": "user", "content": f"""ì‚¬ìš©ì ì§ˆë¬¸: '{original_query}'

ë©”ë‰´ í›„ë³´ë“¤:
{menu_text}

ê°€ì¥ ì í•©í•œ ë©”ë‰´ì˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:"""}
                ],
                max_tokens=10,
                temperature=0.1
            )
            
            selected_num = int(response.choices[0].message.content.strip())
            if 1 <= selected_num <= len(candidates):
                selected_menu = candidates[selected_num - 1]
                print(f"ğŸ¯ AI ìµœì¢… ì„ íƒ: {selected_num}ë²ˆ - {selected_menu['data'].get('page_name', '')}")
                return selected_menu
            else:
                return candidates[0]
                
        except Exception as e:
            print(f"âŒ AI ë©”ë‰´ ì„ íƒ ì‹¤íŒ¨: {e}")
            return candidates[0]
    
    def search(self, query):
        """í†µí•© ê²€ìƒ‰ ìˆ˜í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ í†µí•© AI ê²€ìƒ‰ ì‹œì‘: '{query}'")
        print(f"{'='*60}")
        
        # 1ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        print(f"\nğŸ” 1ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)")
        candidates = self.hybrid_search(query, top_k=7)
        
        if not candidates:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"   ì°¾ì€ í›„ë³´ {len(candidates)}ê°œ:")
        for i, result in enumerate(candidates, 1):
            item = result['data']
            page_name = item.get('page_name', '')
            vector_score = result['vector_score']
            keyword_score = result['keyword_score']
            hybrid_score = result['hybrid_score']
            
            print(f"   {i}. {page_name}")
            print(f"      ë²¡í„°: {vector_score:.3f} | í‚¤ì›Œë“œ: {keyword_score:.3f} | í†µí•©: {hybrid_score:.3f}")
        
        # 2ë‹¨ê³„: AI ìµœì¢… ì„ íƒ
        print(f"\nğŸ¯ 2ë‹¨ê³„: AI ìµœì¢… ì„ íƒ")
        final_result = self.ai_final_selection(query, candidates)
        
        return final_result
    
    def print_result(self, query, result):
        """ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥"""
        if not result:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        item = result['data']
        print(f"\n{'='*60}")
        print(f"ğŸ† ìµœì¢… ì¶”ì²œ ë©”ë‰´")
        print(f"{'='*60}")
        print(f"ğŸ“ í˜ì´ì§€ëª…: {item.get('page_name', '')}")
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {item.get('Category', '')}")
        print(f"ğŸ¢ ì„œë¹„ìŠ¤: {item.get('Service', '')}")
        
        if item.get('hierarchy'):
            print(f"ğŸ“‹ ê³„ì¸µêµ¬ì¡°: {' > '.join(item.get('hierarchy', []))}")
        
        print(f"ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {result.get('hybrid_score', 0):.4f}")
        print(f"   â”£ ë²¡í„° ì ìˆ˜: {result.get('vector_score', 0):.4f}")
        print(f"   â”— í‚¤ì›Œë“œ ì ìˆ˜: {result.get('keyword_score', 0):.4f}")
        
        ai_description = item.get('ai_description', '')
        if ai_description:
            print(f"ğŸ¤– AI ì„¤ëª…: {ai_description}")
        
        representative_keywords = item.get('representative_keywords', [])
        if representative_keywords:
            print(f"ğŸ” ëŒ€í‘œ ê²€ìƒ‰ì–´: {', '.join(representative_keywords)}")
        
        page_classification = item.get('page_classification', '')
        if page_classification:
            print(f"ğŸ“ˆ í˜ì´ì§€ ë¶„ë¥˜: {page_classification}")
        
        print(f"{'='*60}")
    
    def get_statistics(self):
        """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ì‹œìŠ¤í…œ í†µê³„")
        print(f"{'='*60}")
        print(f"ğŸ“„ ì›ë³¸ ë°ì´í„°: {len(self.original_data)}ê°œ í•­ëª©")
        print(f"âœ¨ í•„í„°ë§ëœ ë°ì´í„°: {len(self.filtered_data)}ê°œ í•­ëª©")
        print(f"ğŸš€ ê°•í™”ëœ ë°ì´í„°: {len(self.enhanced_data)}ê°œ í•­ëª©")
        print(f"ğŸ§  ì‚¬ìš© ëª¨ë¸: {self.current_model_name}")
        
        # ë¶„ë¥˜ í†µê³„
        primary_count = sum(1 for item in self.enhanced_data if item.get('page_classification') == 'PRIMARY')
        secondary_count = len(self.enhanced_data) - primary_count
        
        print(f"ğŸ¯ PRIMARY í˜ì´ì§€: {primary_count}ê°œ")
        print(f"ğŸ“‹ SECONDARY í˜ì´ì§€: {secondary_count}ê°œ")
        print(f"ğŸ“ˆ í•„í„°ë§ íš¨ìœ¨ì„±: {((len(self.original_data) - len(self.filtered_data)) / len(self.original_data) * 100):.1f}% ì œê±°")
        print(f"{'='*60}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI ë©”ë‰´ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("ìë™ í•„í„°ë§ + ë°ì´í„° ê°•í™” + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
    
    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        api_key = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not api_key:
            print("âŒ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        searcher = UnifiedMenuSearcher(api_key)
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        searcher.get_statistics()
        
        print("\nìì—°ì–´ë¡œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” ('q' ì…ë ¥ì‹œ ì¢…ë£Œ, 'stats' ì…ë ¥ì‹œ í†µê³„ ì¡°íšŒ):")
        
        while True:
            try:
                query = input("\n> ").strip()
                
                if query.lower() == 'q':
                    print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if query.lower() == 'stats':
                    searcher.get_statistics()
                    continue
                
                if not query:
                    print("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # ê²€ìƒ‰ ìˆ˜í–‰
                result = searcher.search(query)
                searcher.print_result(query, result)
                
            except KeyboardInterrupt:
                print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 