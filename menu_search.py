#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìˆœìˆ˜ AI ê²€ìƒ‰ ì—”ì§„ v3.0 - LLM ì˜ì¡´ì„± ì œê±°
- Bi-encoder (multilingual-e5-small) + BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- Cross-encoder (ms-marco-MiniLM-L-12-v2) Rerank
- 2ë‹¨ê³„ ì•™ìƒë¸” ê²€ìƒ‰ ì‹œìŠ¤í…œ
"""

import json
import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import os
import time
import re
from collections import Counter
import math
import ssl

warnings.filterwarnings("ignore")

# SSL ì¸ì¦ì„œ ë¬¸ì œ í•´ê²°
ssl._create_default_https_context = ssl._create_unverified_context


class BM25:
    """BM25 ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)"""
    
    def __init__(self, corpus, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.corpus = corpus
        self.doc_freqs = []
        self.idf = {}
        self.doc_lens = []
        self.avgdl = 0
        
        self._initialize()
    
    def _initialize(self):
        """BM25 ì´ˆê¸°í™”"""
        for doc in self.corpus:
            words = doc.lower().split()
            self.doc_lens.append(len(words))
            
            word_freq = Counter(words)
            self.doc_freqs.append(word_freq)
        
        self.avgdl = sum(self.doc_lens) / len(self.doc_lens)
        
        all_words = set()
        for doc_freq in self.doc_freqs:
            all_words.update(doc_freq.keys())
        
        for word in all_words:
            containing_docs = sum(1 for doc_freq in self.doc_freqs if word in doc_freq)
            self.idf[word] = math.log((len(self.corpus) - containing_docs + 0.5) / (containing_docs + 0.5))
    
    def get_scores(self, query):
        """ì¿¼ë¦¬ì— ëŒ€í•œ ëª¨ë“  ë¬¸ì„œì˜ BM25 ì ìˆ˜ ê³„ì‚°"""
        query_words = query.lower().split()
        scores = []
        
        for i, doc_freq in enumerate(self.doc_freqs):
            score = 0
            doc_len = self.doc_lens[i]
            
            for word in query_words:
                if word in doc_freq:
                    freq = doc_freq[word]
                    idf = self.idf.get(word, 0)
                    
                    numerator = freq * (self.k1 + 1)
                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)
                    score += idf * (numerator / denominator)
            
            scores.append(score)
        
        return scores


class PureSearchEngine:
    """ìˆœìˆ˜ ê²€ìƒ‰ ì—”ì§„ - LLM ì˜ì¡´ì„± ì—†ìŒ"""
    
    def __init__(self):
        """ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”"""
        print("ğŸš€ ìˆœìˆ˜ AI ê²€ìƒ‰ ì—”ì§„ v3.0 ì´ˆê¸°í™”")
        print("="*50)
        
        # ë°ì´í„° ë¡œë“œ
        self.data = self._load_data()
        
        # í•„ìˆ˜ ëª¨ë¸ ë¡œë“œ (ì‹¤íŒ¨ì‹œ ì¢…ë£Œ)
        self._load_required_models()
        
        # ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
        self._create_search_indices()
        
        print("âœ… ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        try:
            with open('ia-data_enhanced.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # PRIMARY, SECONDARY í˜ì´ì§€ë§Œ ìœ ì§€
            filtered_data = [item for item in data if item.get('page_classification') in ['PRIMARY', 'SECONDARY']]
            
            print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ: {len(filtered_data)}ê°œ í•­ëª©")
            return filtered_data
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            exit(1)
    
    def _load_required_models(self):
        """í•„ìˆ˜ ëª¨ë¸ ë¡œë“œ - ì‹¤íŒ¨ì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ"""
        
        # 1. Bi-encoder ëª¨ë¸ ë¡œë“œ
        bi_encoder_path = "./models/multilingual-e5-small"
        if not os.path.exists(bi_encoder_path):
            print(f"âŒ Bi-encoder ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {bi_encoder_path}")
            exit(1)
        
        try:
            print("ğŸ”„ Bi-encoder ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.bi_encoder = SentenceTransformer(bi_encoder_path)
            print("âœ… multilingual-e5-small ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Bi-encoder ë¡œë“œ ì‹¤íŒ¨: {e}")
            exit(1)
        
        # 2. Cross-encoder ëª¨ë¸ ë¡œë“œ
        cross_encoder_path = "./models/ms-marco-MiniLM-L-12-v2"
        if not os.path.exists(cross_encoder_path):
            print(f"âŒ Cross-encoder ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cross_encoder_path}")
            exit(1)
        
        try:
            print("ğŸ”„ Cross-encoder ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.cross_encoder = CrossEncoder(cross_encoder_path)
            print("âœ… ms-marco-MiniLM-L-12-v2 ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Cross-encoder ë¡œë“œ ì‹¤íŒ¨: {e}")
            exit(1)
    
    def _create_search_indices(self):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±"""
        print("ğŸ” ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # 1. Bi-encoder ì„ë² ë”© ìƒì„±
        texts = []
        for item in self.data:
            # ê°•í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±
            parts = [
                item.get('page_name', ''),
                item.get('Category', ''),
                item.get('Service', ''),
                ' '.join(item.get('hierarchy', [])),
                ' '.join(item.get('representative_keywords', [])),
                item.get('ai_description', '')
            ]
            text = ' '.join(filter(None, parts))
            texts.append(text)
        
        self.embeddings = self.bi_encoder.encode(texts, show_progress_bar=True)
        print(f"âœ… {len(self.embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
        # 2. BM25 ì¸ë±ìŠ¤ ìƒì„±
        bm25_texts = []
        for item in self.data:
            parts = [
                item.get('page_name', ''),
                item.get('Category', ''),
                item.get('Service', ''),
                ' '.join(item.get('representative_keywords', []))
            ]
            text = ' '.join(filter(None, parts))
            bm25_texts.append(text)
        
        self.bm25 = BM25(bm25_texts)
        print("âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def hybrid_search(self, query, top_k=10, vector_weight=0.7, keyword_weight=0.3):
        """1ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Bi-encoder + BM25)"""
        
        # ë²¡í„° ê²€ìƒ‰
        query_embedding = self.bi_encoder.encode([query])
        vector_similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_scores = self.bm25.get_scores(query)
        
        # ì ìˆ˜ ì •ê·œí™”
        max_vector = max(vector_similarities) if max(vector_similarities) > 0 else 1
        max_keyword = max(keyword_scores) if max(keyword_scores) > 0 else 1
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        results = []
        for i, (vector_sim, keyword_score) in enumerate(zip(vector_similarities, keyword_scores)):
            normalized_vector = vector_sim / max_vector
            normalized_keyword = keyword_score / max_keyword
            
            # PRIMARY í˜ì´ì§€ ê°€ì¤‘ì¹˜
            item = self.data[i]
            classification_boost = 1.2 if item.get('page_classification') == 'PRIMARY' else 1.0
            
            hybrid_score = ((normalized_vector * vector_weight) + 
                          (normalized_keyword * keyword_weight)) * classification_boost
            
            results.append({
                'index': i,
                'data': item,
                'vector_score': float(vector_sim),
                'keyword_score': float(keyword_score),
                'hybrid_score': float(hybrid_score),
                'classification_boost': classification_boost
            })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        return results[:top_k]
    
    def rerank_candidates(self, query, candidates, top_k=5):
        """2ë‹¨ê³„: Cross-encoder Rerank"""
        if not candidates:
            return []
        
        print(f"ï¿½ï¿½ Cross-encoder Rerank: {len(candidates)}ê°œ í›„ë³´ ì¬ìˆœìœ„í™”")
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
        query_doc_pairs = []
        for candidate in candidates:
            item = candidate['data']
            # ê°„ê²°í•œ ë¬¸ì„œ í…ìŠ¤íŠ¸ ìƒì„±
            doc_text = f"{item.get('page_name', '')} {item.get('ai_description', '')}"
            query_doc_pairs.append([query, doc_text])
        
        # Cross-encoder ì ìˆ˜ ê³„ì‚°
        rerank_scores = self.cross_encoder.predict(query_doc_pairs)
        
        # ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        min_rerank = min(rerank_scores)
        max_rerank = max(rerank_scores)
        score_range = max_rerank - min_rerank if max_rerank > min_rerank else 1
        
        for i, candidate in enumerate(candidates):
            raw_rerank = float(rerank_scores[i])
            normalized_rerank = (raw_rerank - min_rerank) / score_range
            hybrid_score = candidate['hybrid_score']
            
            # ìµœì¢… ì ìˆ˜: Rerank 70% + Hybrid 30%
            final_score = (normalized_rerank * 0.7) + (hybrid_score * 0.3)
            
            candidate['rerank_score'] = raw_rerank
            candidate['final_score'] = final_score
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì¬ì •ë ¬
        reranked = sorted(candidates, key=lambda x: x['final_score'], reverse=True)
        
        print(f"   ìƒìœ„ {min(top_k, len(reranked))}ê°œ ê²°ê³¼:")
        for i, result in enumerate(reranked[:top_k], 1):
            item = result['data']
            print(f"   {i}. {item.get('page_name', '')} (ìµœì¢…: {result['final_score']:.3f})")
        
        return reranked[:top_k]
    
    def search(self, query):
        """í†µí•© ê²€ìƒ‰ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
        print(f"{'='*60}")
        
        # 1ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        print("\nğŸ” 1ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Bi-encoder + BM25)")
        candidates = self.hybrid_search(query, top_k=10)
        
        if not candidates:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"   í›„ë³´ {len(candidates)}ê°œ ë°œê²¬")
        for i, result in enumerate(candidates[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
            item = result['data']
            print(f"   {i}. {item.get('page_name', '')} (í•˜ì´ë¸Œë¦¬ë“œ: {result['hybrid_score']:.3f})")
        
        # 2ë‹¨ê³„: Cross-encoder Rerank
        print(f"\nğŸ¯ 2ë‹¨ê³„: Cross-encoder Rerank")
        final_results = self.rerank_candidates(query, candidates, top_k=5)
        
        return final_results[0] if final_results else None
    
    def print_result(self, query, result):
        """ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥"""
        if not result:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        item = result['data']
        print(f"\n{'='*60}")
        print(f"ğŸ† ìµœì¢… ê²€ìƒ‰ ê²°ê³¼")
        print(f"{'='*60}")
        print(f"ğŸ“ í˜ì´ì§€ëª…: {item.get('page_name', '')}")
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {item.get('Category', '')}")
        print(f"ğŸ¢ ì„œë¹„ìŠ¤: {item.get('Service', '')}")
        
        if item.get('hierarchy'):
            print(f"ğŸ“‹ ê³„ì¸µêµ¬ì¡°: {' > '.join(item.get('hierarchy', []))}")
        
        # ì ìˆ˜ ì •ë³´
        print(f"ğŸ“Š ìµœì¢… ì ìˆ˜: {result.get('final_score', 0):.4f}")
        print(f"   â”£ Rerank ì ìˆ˜: {result.get('rerank_score', 0):.4f}")
        print(f"   â”— í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {result.get('hybrid_score', 0):.4f}")
        
        if result.get('classification_boost', 1.0) > 1.0:
            print(f"   â­ PRIMARY ê°€ì¤‘ì¹˜ ì ìš©")
        
        if item.get('ai_description'):
            print(f"ğŸ“ ì„¤ëª…: {item.get('ai_description', '')}")
        
        if item.get('representative_keywords'):
            print(f"ğŸ” ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(item.get('representative_keywords', []))}")
        
        print(f"{'='*60}")
    
    def get_statistics(self):
        """ì‹œìŠ¤í…œ í†µê³„"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ì‹œìŠ¤í…œ í†µê³„")
        print(f"{'='*60}")
        print(f"ğŸ“„ ì´ ë°ì´í„°: {len(self.data)}ê°œ í•­ëª©")
        print(f"ğŸ§  Bi-encoder: multilingual-e5-small")
        print(f"ğŸ¯ Cross-encoder: ms-marco-MiniLM-L-12-v2")
        
        # ë¶„ë¥˜ í†µê³„
        primary_count = sum(1 for item in self.data if item.get('page_classification') == 'PRIMARY')
        secondary_count = sum(1 for item in self.data if item.get('page_classification') == 'SECONDARY')
        
        print(f"ğŸ¯ PRIMARY í˜ì´ì§€: {primary_count}ê°œ (ê°€ì¤‘ì¹˜ x1.2)")
        print(f"ğŸ“‹ SECONDARY í˜ì´ì§€: {secondary_count}ê°œ")
        print(f"âš–ï¸ í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜: ë²¡í„° 70% + í‚¤ì›Œë“œ 30%")
        print(f"ğŸ”„ Rerank ê°€ì¤‘ì¹˜: Rerank 70% + í•˜ì´ë¸Œë¦¬ë“œ 30%")
        print(f"{'='*60}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        engine = PureSearchEngine()
        
        # í†µê³„ ì¶œë ¥
        engine.get_statistics()
        
        print("\nìì—°ì–´ë¡œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” ('q' ì…ë ¥ì‹œ ì¢…ë£Œ, 'stats' ì…ë ¥ì‹œ í†µê³„ ì¡°íšŒ):")
        
        while True:
            try:
                query = input("\n> ").strip()
                
                if query.lower() == 'q':
                    print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if query.lower() == 'stats':
                    engine.get_statistics()
                    continue
                
                if not query:
                    print("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # ê²€ìƒ‰ ìˆ˜í–‰
                start_time = time.time()
                result = engine.search(query)
                search_time = time.time() - start_time
                
                engine.print_result(query, result)
                print(f"â±ï¸ ê²€ìƒ‰ ì‹œê°„: {search_time:.2f}ì´ˆ")
                
            except KeyboardInterrupt:
                print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        exit(1)


if __name__ == "__main__":
    main()
