#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë©”ë‰´ ê²€ìƒ‰ í”„ë¡œê·¸ë¨
ë²¡í„° ì„ë² ë”©ì„ í†µí•œ ìœ ì‚¬ë„ ê²€ìƒ‰
"""

import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import os
warnings.filterwarnings("ignore")

# SSL ì¸ì¦ì„œ ë¬¸ì œ í•´ê²°
import ssl
ssl._create_default_https_context = ssl._create_unverified_context


class MenuSearcher:
    def __init__(self, json_file_path):
        """
        ë©”ë‰´ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        """
        # ë‹¤ìš´ë¡œë“œëœ ë¡œì»¬ ëª¨ë¸ ê²½ë¡œë“¤ í™•ì¸ (ì„±ëŠ¥ ìˆœì„œëŒ€ë¡œ)
        local_model_paths = [
            "./models/multilingual-e5-small",  # ğŸ¥‡ ìµœê³  ì„±ëŠ¥ (í‰ê·  ìœ ì‚¬ë„: 0.9448)
            "./models/ko-sroberta-multitask",  # ğŸ¥ˆ í•œêµ­ì–´ íŠ¹í™” (í‰ê·  ìœ ì‚¬ë„: 0.8523)
            "./models/distiluse-base-multilingual-cased-v2",  # ğŸ¥‰ ë¹ ë¥¸ ì†ë„
            "./models/all-mpnet-base-v2",  # ì˜ì–´ ì¤‘ì‹¬ ê³ ì„±ëŠ¥
            "./manual_model",  # ê¸°ì¡´ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸
            "./models/paraphrase-multilingual-MiniLM-L12-v2",  # ë‹¤êµ­ì–´ BERT
            "./local_model",   # HuggingFace Hubìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸
            "./paraphrase-multilingual-MiniLM-L12-v2"  # Gitìœ¼ë¡œ cloneëœ ëª¨ë¸
        ]
        
        model_loaded = False
        self.current_model_name = "Unknown"
        
        # ë¡œì»¬ ëª¨ë¸ë“¤ ìˆœì„œëŒ€ë¡œ ì‹œë„
        for model_path in local_model_paths:
            if os.path.exists(model_path) and os.path.exists(f"{model_path}/pytorch_model.bin"):
                try:
                    model_name = os.path.basename(model_path)
                    print(f"ğŸ”„ {model_name} ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                    self.model = SentenceTransformer(model_path)
                    print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                    
                    # ëª¨ë¸ë³„ ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                    if "multilingual-e5-small" in model_path:
                        print("   ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (í‰ê·  ìœ ì‚¬ë„: 94.48%)")
                    elif "ko-sroberta" in model_path:
                        print("   ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (í‰ê·  ìœ ì‚¬ë„: 85.23%)")
                    elif "distiluse" in model_path:
                        print("   âš¡ ê³ ì† ë‹¤êµ­ì–´ ëª¨ë¸")
                    else:
                        print("   ğŸ“¦ ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸")
                    
                    self.use_tfidf = False
                    self.current_model_name = model_name
                    model_loaded = True
                    break
                except Exception as e:
                    print(f"âŒ {model_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        # ë¡œì»¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì˜¨ë¼ì¸ ì‹œë„
        if not model_loaded:
            try:
                print("ì˜¨ë¼ì¸ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
                print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                self.use_tfidf = False
                self.current_model_name = "all-MiniLM-L6-v2"
            except Exception as e:
                print(f"ì˜¨ë¼ì¸ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ TF-IDFë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.use_tfidf = True
                self.current_model_name = "TF-IDF"
            
        self.data = self.load_data(json_file_path)
        if not self.use_tfidf:
            self.embeddings = self.create_embeddings()
        else:
            self.setup_tfidf()
    
    def load_data(self, json_file_path):
        """
        JSON ë°ì´í„° ë¡œë“œ
        """
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ì´ {len(data)}ê°œì˜ í•­ëª©ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return data
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return []
    
    def setup_tfidf(self):
        """
        TF-IDF ì„¤ì • (ë°±ì—… ë°©ë²•)
        """
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        texts = [self.create_text_for_embedding(item) for item in self.data]
        page_texts = [item.get('page_name', '') for item in self.data]
        full_context_texts = [f"{item.get('Category', '')} {item.get('Service', '')} {' '.join(item.get('hierarchy', []))}" for item in self.data]
        
        self.tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
        self.tfidf_vectorizer_page = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
        self.tfidf_vectorizer_context = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
        
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        self.tfidf_matrix_page = self.tfidf_vectorizer_page.fit_transform(page_texts)
        self.tfidf_matrix_context = self.tfidf_vectorizer_context.fit_transform(full_context_texts)
    
    def create_text_for_embedding(self, item):
        """
        ê° í•­ëª©ì— ëŒ€í•´ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±
        """
        category = item.get('Category', '')
        service = item.get('Service', '')
        page_name = item.get('page_name', '')
        hierarchy = ' > '.join(item.get('hierarchy', []))
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ì¡°í•©
        full_text = f"ì¹´í…Œê³ ë¦¬: {category} ì„œë¹„ìŠ¤: {service} í˜ì´ì§€ëª…: {page_name} ê³„ì¸µêµ¬ì¡°: {hierarchy}"
        return full_text.strip()
    
    def create_embeddings(self):
        """
        ëª¨ë“  í•­ëª©ì— ëŒ€í•´ ë²¡í„° ì„ë² ë”© ìƒì„±
        """
        print("ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        texts = [self.create_text_for_embedding(item) for item in self.data]
        
        # ê° í•„ë“œë³„ë¡œë„ ê°œë³„ ì„ë² ë”© ìƒì„±
        page_texts = [item.get('page_name', '') for item in self.data]
        full_context_texts = [f"{item.get('Category', '')} {item.get('Service', '')} {' '.join(item.get('hierarchy', []))}" for item in self.data]
        
        embeddings = {
            'full': self.model.encode(texts, show_progress_bar=True),
            'page': self.model.encode(page_texts, show_progress_bar=True),
            'context': self.model.encode(full_context_texts, show_progress_bar=True)
        }
        
        print("âœ… ë²¡í„° ì„ë² ë”© ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return embeddings
    
    def search(self, query, top_k=3):
        """
        ê²€ìƒ‰ì–´ì— ëŒ€í•´ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        """
        if not self.data:
            return []
        
        if self.use_tfidf:
            return self.search_with_tfidf(query, top_k)
        else:
            return self.search_with_embeddings(query, top_k)
    
    def search_with_embeddings(self, query, top_k):
        """
        ë²¡í„° ì„ë² ë”©ì„ ì‚¬ìš©í•œ ê²€ìƒ‰
        """
        # ê²€ìƒ‰ì–´ ì„ë² ë”©
        query_embedding_full = self.model.encode([query])
        query_embedding_page = self.model.encode([query])
        query_embedding_context = self.model.encode([query])
        
        # ê° ìœ í˜•ë³„ ìœ ì‚¬ë„ ê³„ì‚°
        similarities_full = cosine_similarity(query_embedding_full, self.embeddings['full'])[0]
        similarities_page = cosine_similarity(query_embedding_page, self.embeddings['page'])[0]
        similarities_context = cosine_similarity(query_embedding_context, self.embeddings['context'])[0]
        
        # ê²°ê³¼ ì¡°í•© (ê°€ì¤‘ì¹˜ ì ìš©)
        results = []
        for i, (sim_full, sim_page, sim_context) in enumerate(zip(similarities_full, similarities_page, similarities_context)):
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
            combined_score = (sim_full * 0.5) + (sim_page * 0.3) + (sim_context * 0.2)
            
            results.append({
                'index': i,
                'data': self.data[i],
                'similarity_full': float(sim_full),
                'similarity_page': float(sim_page),
                'similarity_context': float(sim_context),
                'combined_score': float(combined_score)
            })
        
        # ì¢…í•© ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        results.sort(key=lambda x: x['combined_score'], reverse=True)
        return results[:top_k]
    
    def search_with_tfidf(self, query, top_k):
        """
        TF-IDFë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ (ë°±ì—… ë°©ë²•)
        """
        # ê²€ìƒ‰ì–´ë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜
        query_vector = self.tfidf_vectorizer.transform([query])
        query_vector_page = self.tfidf_vectorizer_page.transform([query])
        query_vector_context = self.tfidf_vectorizer_context.transform([query])
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities_full = cosine_similarity(query_vector, self.tfidf_matrix)[0]
        similarities_page = cosine_similarity(query_vector_page, self.tfidf_matrix_page)[0]
        similarities_context = cosine_similarity(query_vector_context, self.tfidf_matrix_context)[0]
        
        # ê²°ê³¼ ì¡°í•©
        results = []
        for i, (sim_full, sim_page, sim_context) in enumerate(zip(similarities_full, similarities_page, similarities_context)):
            results.append({
                'index': i,
                'data': self.data[i],
                'similarity_full': float(sim_full),
                'similarity_page': float(sim_page),
                'similarity_context': float(sim_context),
                'combined_score': float(sim_full)  # ì „ì²´ ìœ ì‚¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            })
        
        # ì „ì²´ ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        results.sort(key=lambda x: x['combined_score'], reverse=True)
        return results[:top_k]
    
    def print_results(self, query, results):
        """
        ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
        """
        print(f"\nê²€ìƒ‰ì–´: {query}")
        print(f"ì‚¬ìš© ëª¨ë¸: {self.current_model_name}")
        print(f"\nìœ ì‚¬í•œ í•­ëª©ë“¤:")
        
        for i, result in enumerate(results, 1):
            item = result['data']
            print(f"\n{i}. ì „ì²´ ìœ ì‚¬ë„: {result['similarity_full']:.4f}")
            print(f"   - í˜ì´ì§€ë³„ ìœ ì‚¬ë„: {result['similarity_page']:.4f}")
            print(f"   - ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {result['similarity_context']:.4f}")
            if not self.use_tfidf:
                print(f"   - ì¢…í•© ì ìˆ˜: {result['combined_score']:.4f}")
            print(f"   ì¹´í…Œê³ ë¦¬: {item.get('Category', '')}")
            print(f"   ì„œë¹„ìŠ¤: {item.get('Service', '')}")
            print(f"   í˜ì´ì§€ëª…: {item.get('page_name', '')}")


def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    """
    searcher = MenuSearcher('ia-data.json')
    
    print("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” ('q' ì…ë ¥ì‹œ ì¢…ë£Œ):")
    
    while True:
        try:
            query = input("> ").strip()
            
            if query.lower() == 'q':
                print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not query:
                print("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            results = searcher.search(query, top_k=3)
            
            if results:
                searcher.print_results(query, results)
            else:
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print("\n" + "="*50)
            
        except KeyboardInterrupt:
            print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


if __name__ == "__main__":
    main() 