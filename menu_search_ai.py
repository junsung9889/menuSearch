#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenAI APIë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ë©”ë‰´ ê²€ìƒ‰ í”„ë¡œê·¸ë¨
"""

import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import os
from openai import OpenAI
import time
warnings.filterwarnings("ignore")

# SSL ì¸ì¦ì„œ ë¬¸ì œ í•´ê²°
import ssl
ssl._create_default_https_context = ssl._create_unverified_context


class AIMenuSearcher:
    def __init__(self, json_file_path, openai_api_key=None):
        """
        AI ë©”ë‰´ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        """
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = OpenAI(api_key=openai_api_key or os.getenv("OPENAI_API_KEY"))
        
        # ë‹¤ìš´ë¡œë“œëœ ë¡œì»¬ ëª¨ë¸ ê²½ë¡œë“¤ í™•ì¸ (ì„±ëŠ¥ ìˆœì„œëŒ€ë¡œ)
        local_model_paths = [
            "./models/multilingual-e5-small",
            "./models/ko-sroberta-multitask",
            "./models/distiluse-base-multilingual-cased-v2",
            "./models/all-mpnet-base-v2",
            "./manual_model",
            "./models/paraphrase-multilingual-MiniLM-L12-v2",
            "./local_model",
            "./paraphrase-multilingual-MiniLM-L12-v2"
        ]
        
        model_loaded = False
        self.current_model_name = "Unknown"
        
        # ë¡œì»¬ ëª¨ë¸ë“¤ ìˆœì„œëŒ€ë¡œ ì‹œë„
        for model_path in local_model_paths:
            if os.path.exists(model_path) and os.path.exists(f"{model_path}/pytorch_model.bin"):
                try:
                    model_name = os.path.basename(model_path)
                    print(f"ğŸ”„ {model_name} ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                    self.model = SentenceTransformer(model_path)
                    print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                    
                    self.use_tfidf = False
                    self.current_model_name = model_name
                    model_loaded = True
                    break
                except Exception as e:
                    print(f"âŒ {model_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        # ë¡œì»¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì˜¨ë¼ì¸ ì‹œë„
        if not model_loaded:
            try:
                print("ì˜¨ë¼ì¸ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
                print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                self.use_tfidf = False
                self.current_model_name = "all-MiniLM-L6-v2"
            except Exception as e:
                print(f"ì˜¨ë¼ì¸ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ TF-IDFë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.use_tfidf = True
                self.current_model_name = "TF-IDF"
        
        self.data = self.load_data(json_file_path)
        
        # ëŒ€í‘œ ê²€ìƒ‰ì–´ ìƒì„± ë° ë¡œë“œ
        self.enhanced_data_path = json_file_path.replace('.json', '_enhanced.json')
        self.data = self.enhance_data_with_keywords()
        
        if not self.use_tfidf:
            self.embeddings = self.create_embeddings()
        else:
            self.setup_tfidf()
    
    def extract_keyword_with_ai(self, query):
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•´ ìì—°ì–´ ì…ë ¥ì„ ê°€ì¥ ì˜ ìš”ì•½í•˜ëŠ” í•œ ë‹¨ì–´ ìƒì„±
        """
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ìì—°ì–´ ì…ë ¥ì„ ê°€ì¥ ì˜ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ì „ì²´ì ìœ¼ë¡œ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” í•œ ë‹¨ì–´ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. í•„ìš”í•˜ë‹¤ë©´ ì—¬ëŸ¬ ë‹¨ì–´ë¥¼ ì¡°í•©í•´ì„œ ë³µí•©ì–´ë¡œ ë§Œë“¤ì–´ë„ ë©ë‹ˆë‹¤. ì˜ˆ: 'ì¹´ë“œ'+'ì´ìš©ë‚´ì—­'='ì¹´ë“œì´ìš©ë‚´ì—­'. ë°˜ë“œì‹œ í•œ ë‹¨ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒ ë¬¸ì¥ì„ ê°€ì¥ ì˜ ìš”ì•½í•˜ëŠ” í•œ ë‹¨ì–´ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”: '{query}'"}
                ],
                max_tokens=20,
                temperature=0.1
            )
            keyword = response.choices[0].message.content.strip()
            print(f"ğŸ” AI ìš”ì•½ í‚¤ì›Œë“œ: '{query}' â†’ '{keyword}'")
            return keyword
        except Exception as e:
            print(f"âŒ AI ìš”ì•½ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return query  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì¿¼ë¦¬ ë°˜í™˜
    
    def select_best_menu_with_ai(self, original_query, menu_candidates):
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•´ ìµœì¢… ë©”ë‰´ í•˜ë‚˜ ì„ íƒ
        """
        try:
            # ë©”ë‰´ í›„ë³´ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
            menu_text = ""
            for i, result in enumerate(menu_candidates, 1):
                item = result['data']
                menu_text += f"{i}. {item.get('page_name', '')} (ì¹´í…Œê³ ë¦¬: {item.get('Category', '')}, ì„œë¹„ìŠ¤: {item.get('Service', '')})\n"
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•´ ê°€ì¥ ì í•©í•œ ë©”ë‰´ë¥¼ ì„ íƒí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ê³¼ ë©”ë‰´ í›„ë³´ë“¤ì„ ë³´ê³ , ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë©”ë‰´ í•˜ë‚˜ì˜ ë²ˆí˜¸ë§Œ ë‹µí•˜ì„¸ìš”. ë°˜ë“œì‹œ ìˆ«ìë§Œ ë‹µí•˜ì„¸ìš”."},
                    {"role": "user", "content": f"ì‚¬ìš©ì ì§ˆë¬¸: '{original_query}'\n\në©”ë‰´ í›„ë³´ë“¤:\n{menu_text}\n\nê°€ì¥ ì í•©í•œ ë©”ë‰´ì˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:"}
                ],
                max_tokens=10,
                temperature=0.1
            )
            
            selected_num = int(response.choices[0].message.content.strip())
            if 1 <= selected_num <= len(menu_candidates):
                selected_menu = menu_candidates[selected_num - 1]
                print(f"ğŸ¯ AI ìµœì¢… ì„ íƒ: {selected_num}ë²ˆ - {selected_menu['data'].get('page_name', '')}")
                return selected_menu
            else:
                print(f"âš ï¸ AI ì„ íƒ ë²ˆí˜¸ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨: {selected_num}")
                return menu_candidates[0]  # ì²« ë²ˆì§¸ í›„ë³´ ë°˜í™˜
                
        except Exception as e:
            print(f"âŒ AI ë©”ë‰´ ì„ íƒ ì‹¤íŒ¨: {e}")
            return menu_candidates[0]  # ì‹¤íŒ¨ì‹œ ì²« ë²ˆì§¸ í›„ë³´ ë°˜í™˜
    
    def load_data(self, json_file_path):
        """JSON ë°ì´í„° ë¡œë“œ"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ì´ {len(data)}ê°œì˜ í•­ëª©ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return data
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return []
    
    def setup_tfidf(self):
        """TF-IDF ì„¤ì • (ë°±ì—… ë°©ë²•)"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        texts = [self.create_text_for_embedding(item) for item in self.data]
        page_texts = [item.get('page_name', '') for item in self.data]
        full_context_texts = [f"{item.get('Category', '')} {item.get('Service', '')} {' '.join(item.get('hierarchy', []))}" for item in self.data]
        
        self.tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
        self.tfidf_vectorizer_page = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
        self.tfidf_vectorizer_context = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
        
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        self.tfidf_matrix_page = self.tfidf_vectorizer_page.fit_transform(page_texts)
        self.tfidf_matrix_context = self.tfidf_vectorizer_context.fit_transform(full_context_texts)
    
    def create_text_for_embedding(self, item):
        """ê° í•­ëª©ì— ëŒ€í•´ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„± (ëŒ€í‘œ ê²€ìƒ‰ì–´ ê°€ì¤‘ì¹˜ í¬í•¨)"""
        category = item.get('Category', '')
        service = item.get('Service', '')
        page_name = item.get('page_name', '')
        hierarchy = ' > '.join(item.get('hierarchy', []))
        
        # ê¸°ë³¸ í…ìŠ¤íŠ¸
        basic_text = f"ì¹´í…Œê³ ë¦¬: {category} ì„œë¹„ìŠ¤: {service} í˜ì´ì§€ëª…: {page_name} ê³„ì¸µêµ¬ì¡°: {hierarchy}"
        
        # ëŒ€í‘œ ê²€ìƒ‰ì–´ê°€ ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ë¡œ ì¶”ê°€ (3ë²ˆ ë°˜ë³µìœ¼ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬)
        representative_keywords = item.get('representative_keywords', [])
        if representative_keywords:
            keywords_text = ' '.join(representative_keywords)
            # ëŒ€í‘œ ê²€ìƒ‰ì–´ë¥¼ 3ë²ˆ ë°˜ë³µí•˜ì—¬ ê°€ì¤‘ì¹˜ ë¶€ì—¬
            weighted_keywords = f" {keywords_text} {keywords_text} {keywords_text}"
            full_text = basic_text + weighted_keywords
        else:
            full_text = basic_text
        
        return full_text.strip()
    
    def create_embeddings(self):
        """ëª¨ë“  í•­ëª©ì— ëŒ€í•´ ë²¡í„° ì„ë² ë”© ìƒì„±"""
        print("ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        texts = [self.create_text_for_embedding(item) for item in self.data]
        
        page_texts = [item.get('page_name', '') for item in self.data]
        full_context_texts = [f"{item.get('Category', '')} {item.get('Service', '')} {' '.join(item.get('hierarchy', []))}" for item in self.data]
        
        embeddings = {
            'full': self.model.encode(texts, show_progress_bar=True),
            'page': self.model.encode(page_texts, show_progress_bar=True),
            'context': self.model.encode(full_context_texts, show_progress_bar=True)
        }
        
        print("âœ… ë²¡í„° ì„ë² ë”© ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return embeddings
    
    def search_with_embeddings(self, query, top_k):
        """ë²¡í„° ì„ë² ë”©ì„ ì‚¬ìš©í•œ ê²€ìƒ‰"""
        # ê²€ìƒ‰ì–´ ì„ë² ë”©
        query_embedding_full = self.model.encode([query])
        query_embedding_page = self.model.encode([query])
        query_embedding_context = self.model.encode([query])
        
        # ê° ìœ í˜•ë³„ ìœ ì‚¬ë„ ê³„ì‚°
        similarities_full = cosine_similarity(query_embedding_full, self.embeddings['full'])[0]
        similarities_page = cosine_similarity(query_embedding_page, self.embeddings['page'])[0]
        similarities_context = cosine_similarity(query_embedding_context, self.embeddings['context'])[0]
        
        # ê²°ê³¼ ì¡°í•© (ê°€ì¤‘ì¹˜ ì ìš©)
        results = []
        for i, (sim_full, sim_page, sim_context) in enumerate(zip(similarities_full, similarities_page, similarities_context)):
            combined_score = (sim_full * 0.5) + (sim_page * 0.3) + (sim_context * 0.2)
            
            results.append({
                'index': i,
                'data': self.data[i],
                'similarity_full': float(sim_full),
                'similarity_page': float(sim_page),
                'similarity_context': float(sim_context),
                'combined_score': float(combined_score)
            })
        
        results.sort(key=lambda x: x['combined_score'], reverse=True)
        return results[:top_k]
    
    def search_with_tfidf(self, query, top_k):
        """TF-IDFë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ (ë°±ì—… ë°©ë²•)"""
        query_vector = self.tfidf_vectorizer.transform([query])
        query_vector_page = self.tfidf_vectorizer_page.transform([query])
        query_vector_context = self.tfidf_vectorizer_context.transform([query])
        
        similarities_full = cosine_similarity(query_vector, self.tfidf_matrix)[0]
        similarities_page = cosine_similarity(query_vector_page, self.tfidf_matrix_page)[0]
        similarities_context = cosine_similarity(query_vector_context, self.tfidf_matrix_context)[0]
        
        results = []
        for i, (sim_full, sim_page, sim_context) in enumerate(zip(similarities_full, similarities_page, similarities_context)):
            results.append({
                'index': i,
                'data': self.data[i],
                'similarity_full': float(sim_full),
                'similarity_page': float(sim_page),
                'similarity_context': float(sim_context),
                'combined_score': float(sim_full)
            })
        
        results.sort(key=lambda x: x['combined_score'], reverse=True)
        return results[:top_k]
    
    def ai_search(self, query):
        """
        AI ê°•í™” ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤
        1. AIë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        2. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìƒìœ„ 5ê°œ í›„ë³´ ì¶”ì¶œ
        3. AIë¡œ ìµœì¢… ë©”ë‰´ í•˜ë‚˜ ì„ íƒ
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ AI ê²€ìƒ‰ ì‹œì‘: '{query}'")
        print(f"{'='*60}")
        
        # 1ë‹¨ê³„: AIë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        print(f"\nğŸ“ 1ë‹¨ê³„: ìš”ì•½ í‚¤ì›Œë“œ ìƒì„±")
        keyword = self.extract_keyword_with_ai(query)
        
        # 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìƒìœ„ 5ê°œ í›„ë³´ ì¶”ì¶œ
        print(f"\nğŸ” 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ (ìƒìœ„ 5ê°œ í›„ë³´)")
        if self.use_tfidf:
            candidates = self.search_with_tfidf(keyword, top_k=5)
        else:
            candidates = self.search_with_embeddings(keyword, top_k=5)
        
        if not candidates:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"   ì°¾ì€ í›„ë³´ {len(candidates)}ê°œ:")
        for i, result in enumerate(candidates, 1):
            item = result['data']
            print(f"   {i}. {item.get('page_name', '')} (ìœ ì‚¬ë„: {result['combined_score']:.4f})")
        
        # 3ë‹¨ê³„: AIë¡œ ìµœì¢… ë©”ë‰´ ì„ íƒ
        print(f"\nğŸ¯ 3ë‹¨ê³„: AI ìµœì¢… ì„ íƒ")
        final_result = self.select_best_menu_with_ai(query, candidates)
        
        return final_result
    
    def print_final_result(self, query, result):
        """ìµœì¢… ê²°ê³¼ ì¶œë ¥"""
        if not result:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        item = result['data']
        print(f"\n{'='*60}")
        print(f"ğŸ† ìµœì¢… ì¶”ì²œ ë©”ë‰´")
        print(f"{'='*60}")
        print(f"ğŸ“ í˜ì´ì§€ëª…: {item.get('page_name', '')}")
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {item.get('Category', '')}")
        print(f"ğŸ¢ ì„œë¹„ìŠ¤: {item.get('Service', '')}")
        if item.get('hierarchy'):
            print(f"ğŸ“‹ ê³„ì¸µêµ¬ì¡°: {' > '.join(item.get('hierarchy', []))}")
        print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {result['combined_score']:.4f}")
        
        # ëŒ€í‘œ ê²€ìƒ‰ì–´ ì •ë³´ ì¶œë ¥
        representative_keywords = item.get('representative_keywords', [])
        if representative_keywords:
            print(f"ğŸ” AI ìƒì„± ëŒ€í‘œ ê²€ìƒ‰ì–´: {', '.join(representative_keywords)}")
        
        print(f"{'='*60}")
    
    def generate_representative_keywords(self, item):
        """
        íŠ¹ì • ë©”ë‰´ í•­ëª©ì— ëŒ€í•´ OpenAIë¡œë¶€í„° ëŒ€í‘œ ê²€ìƒ‰ì–´ 5ê°œ ìƒì„±
        """
        try:
            category = item.get('Category', '')
            service = item.get('Service', '')
            page_name = item.get('page_name', '')
            hierarchy = ' > '.join(item.get('hierarchy', []))
            
            menu_info = f"ì¹´í…Œê³ ë¦¬: {category}, ì„œë¹„ìŠ¤: {service}, í˜ì´ì§€ëª…: {page_name}, ê³„ì¸µêµ¬ì¡°: {hierarchy}"
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë©”ë‰´ ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë©”ë‰´ ì •ë³´ë¥¼ ë³´ê³ , ì‚¬ìš©ìê°€ ì´ ë©”ë‰´ë¥¼ ì°¾ê¸° ìœ„í•´ ê²€ìƒ‰í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²€ìƒ‰ì–´ 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ê° ê²€ìƒ‰ì–´ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”. ì‹¤ì œ ì‚¬ìš©ìë“¤ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•  ë²•í•œ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒ ë©”ë‰´ì— ëŒ€í•œ ëŒ€í‘œ ê²€ìƒ‰ì–´ 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n{menu_info}"}
                ],
                max_tokens=100,
                temperature=0.3
            )
            
            keywords_text = response.choices[0].message.content.strip()
            keywords = [kw.strip() for kw in keywords_text.split(',')]
            
            # ì •í™•íˆ 5ê°œê°€ ì•„ë‹ˆë©´ ì¡°ì •
            if len(keywords) > 5:
                keywords = keywords[:5]
            elif len(keywords) < 5:
                # ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë“¤ë¡œ ì±„ì›€
                basic_keywords = [category, service, page_name]
                for bk in basic_keywords:
                    if bk and bk not in keywords and len(keywords) < 5:
                        keywords.append(bk)
            
            return keywords[:5]
            
        except Exception as e:
            print(f"âŒ ëŒ€í‘œ ê²€ìƒ‰ì–´ ìƒì„± ì‹¤íŒ¨ ({item.get('page_name', 'Unknown')}): {e}")
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
            return [
                item.get('page_name', ''),
                item.get('Category', ''),
                item.get('Service', ''),
                ' '.join(item.get('hierarchy', [])),
                f"{item.get('Category', '')} {item.get('Service', '')}"
            ]
    
    def enhance_data_with_keywords(self):
        """
        ë©”ë‰´ ë°ì´í„°ì— ëŒ€í‘œ ê²€ìƒ‰ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê°•í™”ëœ ë°ì´í„° ìƒì„±
        """
        # ì´ë¯¸ ê°•í™”ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(self.enhanced_data_path):
            try:
                with open(self.enhanced_data_path, 'r', encoding='utf-8') as f:
                    enhanced_data = json.load(f)
                print(f"âœ… ê¸°ì¡´ ê°•í™”ëœ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {len(enhanced_data)}ê°œ í•­ëª©")
                return enhanced_data
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ê°•í™”ëœ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ¤– AIë¥¼ í™œìš©í•´ ê° ë©”ë‰´ë³„ ëŒ€í‘œ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        print(f"ğŸ“Š ì´ {len(self.data)}ê°œ í•­ëª© ì²˜ë¦¬ ì˜ˆì • (ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤)")
        
        enhanced_data = []
        
        for i, item in enumerate(self.data, 1):
            print(f"ğŸ”„ ì§„í–‰ì¤‘ [{i}/{len(self.data)}]: {item.get('page_name', 'Unknown')}")
            
            # ëŒ€í‘œ ê²€ìƒ‰ì–´ ìƒì„±
            representative_keywords = self.generate_representative_keywords(item)
            
            # ê¸°ì¡´ ë°ì´í„°ì— ê²€ìƒ‰ì–´ ì¶”ê°€
            enhanced_item = item.copy()
            enhanced_item['representative_keywords'] = representative_keywords
            enhanced_data.append(enhanced_item)
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ë”œë ˆì´
            if i % 10 == 0:
                print(f"â±ï¸ API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•´ ì ì‹œ ëŒ€ê¸°ì¤‘...")
                time.sleep(2)
            else:
                time.sleep(0.5)
        
        # ê°•í™”ëœ ë°ì´í„° ì €ì¥
        try:
            with open(self.enhanced_data_path, 'w', encoding='utf-8') as f:
                json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ê°•í™”ëœ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {self.enhanced_data_path}")
        except Exception as e:
            print(f"âŒ ê°•í™”ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return enhanced_data


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¤– AI ê°•í™” ë©”ë‰´ ê²€ìƒ‰ í”„ë¡œê·¸ë¨")
    print("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        api_key = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not api_key:
            print("âŒ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
    
    try:
        searcher = AIMenuSearcher('ia-data.json', api_key)
        print(f"\nâœ… ì´ˆê¸°í™” ì™„ë£Œ! ì‚¬ìš© ëª¨ë¸: {searcher.current_model_name}")
        print("ìì—°ì–´ë¡œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” ('q' ì…ë ¥ì‹œ ì¢…ë£Œ):")
        
        while True:
            try:
                query = input("\n> ").strip()
                
                if query.lower() == 'q':
                    print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not query:
                    print("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # AI ê²€ìƒ‰ ìˆ˜í–‰
                result = searcher.ai_search(query)
                searcher.print_final_result(query, result)
                
            except KeyboardInterrupt:
                print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


if __name__ == "__main__":
    main() 